{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T13:58:28.748660Z",
     "start_time": "2020-06-04T13:58:27.568742Z"
    }
   },
   "outputs": [],
   "source": [
    "from petastorm import make_batch_reader, TransformSpec\n",
    "from petastorm.pytorch import DataLoader as PetaDataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader as TorchDataLoader, IterableDataset\n",
    "from sklearn import preprocessing\n",
    "from collections import OrderedDict\n",
    "from torch import tensor\n",
    "import math\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T13:58:28.773774Z",
     "start_time": "2020-06-04T13:58:28.751086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching\n"
     ]
    }
   ],
   "source": [
    "FILE_PREFIX = 'file:'\n",
    "\n",
    "pre_open_fds = None\n",
    "def patch_leaking_fd():\n",
    "    global pre_open_fds\n",
    "    from pyarrow.parquet import ParquetFile, ParquetReader\n",
    "    def _patched_init(self, source, **kwargs):\n",
    "        self.source = source\n",
    "        return ParquetFile.__old_init__(self, source, **kwargs)\n",
    "\n",
    "    def _exit(self, *args, **kwargs):\n",
    "        if hasattr(self.source, 'close'):\n",
    "            self.source.close()\n",
    "            del self.source\n",
    "\n",
    "    def _bopen(fn):    \n",
    "        return open(fn, 'rb')\n",
    "\n",
    "    pre_open_fds = _bopen\n",
    "    if not hasattr(ParquetFile, '__old_init__'):\n",
    "        print(\"Patching\")\n",
    "        ParquetFile.__old_init__ = ParquetFile.__init__\n",
    "\n",
    "        ParquetFile.__init__ = _patched_init\n",
    "        ParquetFile.__exit__ = _exit\n",
    "        ParquetFile.__del__ = _exit\n",
    "\n",
    "    else:\n",
    "        print(\"Already patched\")\n",
    "\n",
    "patch_leaking_fd()\n",
    "\n",
    "\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "    def __init__(self, filename, rex=None):\n",
    "        super(MyIterableDataset).__init__()\n",
    "        self._filename_param = filename\n",
    "        self.filename = self._init_filenames(filename, rex)\n",
    "\n",
    "    def _init_filenames(self, filename, rex):\n",
    "        if rex is None:\n",
    "            return filename\n",
    "        \n",
    "        filename = filename[len(FILE_PREFIX):]\n",
    "        if not os.path.isdir(filename):\n",
    "            raise ValueError(f\"Filtering only possible for dirs, {filename} is not a one\")\n",
    "        paths = [os.path.join(dp, f) for dp, dn, fn in os.walk(filename) for f in fn]\n",
    "        res = list(map(\n",
    "            lambda f: FILE_PREFIX + f,\n",
    "            filter(lambda f: re.match(rex, f) is not None, paths)\n",
    "        ))\n",
    "        if (len(res) == 0):\n",
    "            raise ValueError(f\"0 files remained out ot {len(paths)} - seems regex is too restrictive\")\n",
    "\n",
    "        return res;\n",
    "\n",
    "    def _init_petaloader(self):\n",
    "        def _transform_row(df_batch):\n",
    "            return df_batch\n",
    "\n",
    "        transform = TransformSpec(_transform_row, removed_fields=['cat_id', 'store_id', 'state_id'])\n",
    "        reader = make_batch_reader(self.filename,\n",
    "                 schema_fields=['id', 'item_id', 'dept_id', 'cat_id', 'day_id',\n",
    "               'sales', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
    "               'snap_flag', 'sell_price', 'sales_dollars', 'store_id', 'state_id'],\n",
    "                workers_count=1\n",
    "                #,transform_spec = transform\n",
    "        )\n",
    "        return PetaDataLoader(reader=reader, batch_size=128, shuffling_queue_capacity=100000)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1913*30490 # can be arbitrary large value to prevent WARN logs, seem to be ignored anyway\n",
    "\n",
    "    def __iter__(self):\n",
    "        print(f\"Iterator created on {self._filename_param}\")\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            count_cells = 0\n",
    "            count_batches = 0\n",
    "            with self._init_petaloader() as loader:\n",
    "                if pre_open_fds:\n",
    "                    loader.reader.dataset.fs.open = pre_open_fds\n",
    "                for batch in loader:\n",
    "                    count_batches += 1\n",
    "                    for price, sales_dollars in zip(batch['sell_price'], batch['sales_dollars']):\n",
    "                        price_is_nan = math.isnan(price)\n",
    "                        price_or_zero = 0. if price_is_nan else price\n",
    "                        count_cells += 1\n",
    "                        yield {'features': tensor([price_or_zero, price_is_nan]),\n",
    "                               'targets': tensor([sales_dollars])}\n",
    "                        \n",
    "            print(f'Done iterating: {count_batches} batches / ({count_cells} cells) ')\n",
    "        else:\n",
    "            raise ValueError(\"Not implemented for multithreading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T13:58:29.554922Z",
     "start_time": "2020-06-04T13:58:28.777001Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.utils import set_global_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T13:58:29.563941Z",
     "start_time": "2020-06-04T13:58:29.558555Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED=42\n",
    "set_global_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T13:58:29.571204Z",
     "start_time": "2020-06-04T13:58:29.566759Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = 128\n",
    "\n",
    "train_ds = MyIterableDataset('file:./sales_series_melt.parquet/parquet_partition=2')\n",
    "valid_ds = MyIterableDataset('file:./sales_series_melt.parquet/parquet_partition=1')\n",
    "\n",
    "train_dl = TorchDataLoader(train_ds, batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "valid_dl = TorchDataLoader(valid_ds, batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "data = OrderedDict()\n",
    "data[\"train\"] = train_dl\n",
    "data[\"valid\"] = valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T14:17:35.932875Z",
     "start_time": "2020-06-04T14:17:35.925320Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Sequential):\n",
    "    def __init__(self, num_features):\n",
    "        layers = []\n",
    "        layer_dims = [num_features, 200,200,20,20,1]\n",
    "        for in_features, out_features in zip(layer_dims[:-1], layer_dims[1:]):\n",
    "            l = nn.Linear(in_features, out_features)\n",
    "            # Note to self: loss @ init is quite important!\n",
    "            torch.nn.init.xavier_uniform_(l.weight) \n",
    "            torch.nn.init.zeros_(l.bias)\n",
    "\n",
    "            layers.append(l)\n",
    "            layers.append(nn.ReLU())\n",
    "        super(Net, self).__init__(*layers)\n",
    "\n",
    "class MyLoss(nn.MSELoss):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        return super().forward(inp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T14:27:45.407753Z",
     "start_time": "2020-06-04T14:27:45.403314Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Net(num_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T14:27:45.542939Z",
     "start_time": "2020-06-04T14:27:45.539331Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = MyLoss()\n",
    "runner = SupervisedRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T14:27:49.030643Z",
     "start_time": "2020-06-04T14:27:45.689220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator created on file:./sales_series_melt.parquet/parquet_partition=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0081, 0.0373, 0.0135, 0.0975, 0.0367, 0.0526, 0.0225, 0.0646, 0.0324,\n",
       "         0.0121, 0.0308, 0.0184, 0.0077, 0.0544, 0.0204, 0.0482, 0.0730, 0.0808,\n",
       "         0.0000, 0.0000, 0.0235, 0.0000, 0.0243, 0.0568, 0.0081, 0.0000, 0.0568,\n",
       "         0.0251, 0.0000, 0.0267, 0.0446, 0.0135, 0.0081, 0.0163, 0.0511, 0.0081,\n",
       "         0.0000, 0.0243, 0.0000, 0.0242, 0.0468, 0.0204, 0.0000, 0.0079, 0.0000,\n",
       "         0.1178, 0.0104, 0.0169, 0.0242, 0.0894, 0.0161, 0.0267, 0.0323, 0.0590,\n",
       "         0.0284, 0.0161, 0.0204, 0.0161, 0.0243, 0.0588, 0.0204, 0.0397, 0.0470,\n",
       "         0.0321, 0.0313, 0.0812, 0.0161, 0.0000, 0.0266, 0.0243, 0.0080, 0.0178,\n",
       "         0.0000, 0.0000, 0.0365, 0.0000, 0.0000, 0.0650, 0.0243, 0.0299, 0.0266,\n",
       "         0.1220, 0.0080, 0.0813, 0.0000, 0.0568, 0.0161, 0.0000, 0.0402, 0.0323,\n",
       "         0.0385, 0.0000, 0.0242, 0.0000, 0.0000, 0.0997, 0.0000, 0.0145, 0.0227,\n",
       "         0.0000, 0.0243, 0.0000, 0.0650, 0.0323, 0.0484, 0.0235, 0.0000, 0.0315,\n",
       "         0.0000, 0.0209, 0.0161, 0.0000, 0.0437, 0.0320, 0.0235, 0.0324, 0.0285,\n",
       "         0.0649, 0.0241, 0.0323, 0.0000, 0.0000, 0.0271, 0.0000, 0.0081, 0.0234,\n",
       "         0.0037, 0.0731]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_batch = next(iter(train_dl))\n",
    "model.forward(trn_batch['features']).transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T14:27:52.550841Z",
     "start_time": "2020-06-04T14:27:49.132374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator created on file:./sales_series_melt.parquet/parquet_partition=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0204, 0.0226, 0.0093, 0.0284, 0.0000, 0.0894, 0.0000, 0.0161, 0.0000,\n",
       "         0.0080, 0.0402, 0.0363, 0.0204, 0.0000, 0.0161, 0.0000, 0.0088, 0.0285,\n",
       "         0.0079, 0.0000, 0.0732, 0.0511, 0.0142, 0.0000, 0.0000, 0.0227, 0.0715,\n",
       "         0.0640, 0.0324, 0.1057, 0.0000, 0.0234, 0.0284, 0.0210, 0.0204, 0.0894,\n",
       "         0.1220, 0.0715, 0.0055, 0.0349, 0.0201, 0.0227, 0.0161, 0.0258, 0.0161,\n",
       "         0.0000, 0.0728, 0.0080, 0.0161, 0.1628, 0.1290, 0.0487, 0.0772, 0.0649,\n",
       "         0.0044, 0.0324, 0.0242, 0.0201, 0.0202, 0.0243, 0.0731, 0.0227, 0.0313,\n",
       "         0.0079, 0.0079, 0.0568, 0.0242, 0.0161, 0.0019, 0.0000, 0.0406, 0.0077,\n",
       "         0.0363, 0.0242, 0.0047, 0.0079, 0.0895, 0.0402, 0.0487, 0.0202, 0.0638,\n",
       "         0.0101, 0.0000, 0.0243, 0.0000, 0.0487, 0.0000, 0.0405, 0.0242, 0.0975,\n",
       "         0.0081, 0.0275, 0.0161, 0.0649, 0.1058, 0.0308, 0.0034, 0.0405, 0.0242,\n",
       "         0.0243, 0.0161, 0.0528, 0.0486, 0.0000, 0.0186, 0.0243, 0.0299, 0.0292,\n",
       "         0.0079, 0.0202, 0.0365, 0.0813, 0.0157, 0.0201, 0.0129, 0.0000, 0.0398,\n",
       "         0.0000, 0.0405, 0.0194, 0.0486, 0.0324, 0.0235, 0.0527, 0.0079, 0.0218,\n",
       "         0.0000, 0.0243]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_batch = next(iter(valid_dl))\n",
    "model.forward(valid_batch['features']).transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T14:28:56.323723Z",
     "start_time": "2020-06-04T14:27:52.653356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator created on file:./sales_series_melt.parquet/parquet_partition=2\n",
      "Done iterating: 3047 batches / (389957 cells) \n",
      "Iterator created on file:./sales_series_melt.parquet/parquet_partition=1\n",
      "Done iterating: 3042 batches / (389299 cells) \n",
      "[2020-06-04 07:28:56,298] \n",
      "1/1 * Epoch 1 (_base): lr=0.0100 | momentum=0.9000\n",
      "1/1 * Epoch 1 (train): loss=94.0748\n",
      "1/1 * Epoch 1 (valid): loss=89.3520\n",
      "Top best models:\n",
      "run/checkpoints/train.1.pth\t89.3520\n",
      "=> Loading checkpoint run/checkpoints/best_full.pth\n",
      "loaded state checkpoint run/checkpoints/best_full.pth (global epoch 1, epoch 1, stage train)\n"
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=data,\n",
    "    logdir=\"run\",\n",
    "    load_best_on_end=True,\n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T14:28:56.520113Z",
     "start_time": "2020-06-04T14:28:56.507857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.4297, 3.3355, 3.9812, 5.1528, 3.2807, 3.8872, 2.7261, 4.2225, 2.7297,\n",
       "         4.3285, 2.7261, 2.7261, 3.0483, 3.9366, 2.7261, 3.7669, 4.4599, 4.6809,\n",
       "         0.0000, 0.0000, 2.7261, 0.0000, 2.7261, 4.0017, 3.4297, 0.0000, 4.0017,\n",
       "         2.7261, 0.0000, 2.7261, 3.6705, 3.9812, 3.4297, 3.0932, 3.8445, 3.4297,\n",
       "         0.0000, 2.7261, 0.0000, 2.7261, 3.7284, 2.7261, 0.0000, 3.2584, 0.0000,\n",
       "         5.7260, 4.5804, 2.7261, 2.7261, 4.9226, 3.1384, 2.7261, 2.7261, 4.0627,\n",
       "         2.7261, 3.1384, 2.7261, 3.1384, 2.7261, 4.0580, 2.7261, 3.5187, 3.7348,\n",
       "         2.7261, 2.7261, 4.6924, 3.1581, 0.0000, 2.7261, 2.7261, 3.3166, 2.7261,\n",
       "         0.0000, 0.0000, 3.2669, 0.0000, 0.0000, 4.2341, 2.7261, 2.7261, 2.7261,\n",
       "         5.8434, 3.3166, 4.6947, 0.0000, 4.0017, 3.1581, 0.0000, 3.5567, 2.7261,\n",
       "         3.4284, 0.0000, 2.7261, 0.0000, 0.0000, 5.2149, 0.0000, 3.6469, 2.7261,\n",
       "         0.0000, 2.7261, 0.0000, 4.2341, 2.7261, 3.7712, 2.7261, 0.0000, 2.7261,\n",
       "         0.0000, 2.7261, 3.1384, 0.0000, 3.6470, 2.7261, 2.7261, 2.7297, 2.7261,\n",
       "         4.2318, 2.7261, 2.7261, 0.0000, 0.0000, 2.7261, 0.0000, 3.4297, 2.7261,\n",
       "         2.7261, 4.4622]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(trn_batch['features']).transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T14:28:56.706962Z",
     "start_time": "2020-06-04T14:28:56.690940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7261, 2.7261, 4.1698, 2.7261, 0.0000, 4.9226, 0.0000, 3.1384, 0.0000,\n",
       "         3.3166, 3.5567, 3.2532, 2.7261, 0.0000, 3.1384, 0.0000, 3.8799, 2.7261,\n",
       "         3.2584, 0.0000, 4.4645, 3.8445, 3.7573, 0.0000, 0.0000, 2.7261, 4.4162,\n",
       "         4.2063, 2.7297, 5.3830, 0.0000, 2.7261, 2.7261, 2.7261, 2.7261, 4.9226,\n",
       "         5.8434, 4.4162, 2.7261, 3.1372, 2.7261, 2.7261, 3.1581, 2.7261, 3.1581,\n",
       "         0.0000, 4.4553, 3.3166, 3.1384, 6.9966, 6.0413, 3.7798, 4.5796, 4.2318,\n",
       "         2.7261, 2.7297, 2.7261, 2.7261, 2.7261, 2.7261, 4.4622, 2.7261, 2.7261,\n",
       "         3.2584, 3.2584, 4.0017, 2.7261, 3.1384, 2.7261, 0.0000, 3.5656, 3.0483,\n",
       "         3.2532, 2.7261, 2.7261, 3.2584, 4.9249, 3.5567, 3.7798, 2.7261, 4.1994,\n",
       "         4.5478, 0.0000, 2.7261, 0.0000, 3.7798, 0.0000, 3.5634, 2.7261, 5.1528,\n",
       "         3.4297, 2.7261, 3.1384, 4.2318, 5.3853, 2.7261, 2.7261, 3.5634, 2.7261,\n",
       "         2.7261, 3.1384, 3.8917, 3.7777, 0.0000, 2.7261, 2.7261, 2.7261, 2.7261,\n",
       "         3.2584, 2.7261, 3.2669, 4.6947, 3.2428, 2.7261, 4.1500, 0.0000, 3.5242,\n",
       "         0.0000, 3.5634, 2.7261, 3.7777, 2.7297, 2.7261, 3.8894, 3.2584, 2.7261,\n",
       "         0.0000, 2.7261]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(valid_batch['features']).transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T14:29:00.169344Z",
     "start_time": "2020-06-04T14:28:56.840582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator created on file:./sales_series_melt.parquet/parquet_partition=1\n",
      "tensor([[3.5955, 5.1528, 2.7261, 3.1384, 2.7261, 2.7261, 3.2584, 2.7261, 4.2248,\n",
      "         2.7261, 2.7261, 2.7261, 3.0483, 3.6469, 2.7261, 3.7584, 3.8782, 0.0000,\n",
      "         3.5567, 2.7261, 4.2341, 4.1160, 3.3714, 3.7027, 3.7573, 0.0000, 4.9249,\n",
      "         3.4297, 3.0483, 4.2318, 3.6384, 2.7261, 0.0000, 3.7798, 3.4019, 2.7261,\n",
      "         3.5656, 2.7261, 2.7261, 2.7261, 2.7297, 3.5567, 3.5634, 3.9793, 3.2584,\n",
      "         2.7261, 2.7261, 3.7712, 2.7297, 0.0000, 4.3285, 0.0000, 3.3287, 2.7261,\n",
      "         3.4683, 2.7261, 3.1581, 3.7798, 4.1183, 3.1384, 0.0000, 3.9276, 3.5634,\n",
      "         0.0000, 2.7261, 3.2584, 2.7261, 2.7261, 2.7261, 2.7261, 2.7261, 0.0000,\n",
      "         0.0000, 3.2584, 0.0000, 2.7261, 2.7261, 4.0039, 2.7297, 3.8378, 3.7070,\n",
      "         3.6194, 0.0000, 3.7798, 3.8872, 3.8799, 2.7261, 2.7261, 3.2807, 3.1581,\n",
      "         2.7261, 3.1372, 2.7261, 2.7261, 2.7261, 3.5634, 0.0000, 3.1384, 3.7798,\n",
      "         4.0017, 2.7261, 3.5242, 3.2584, 3.8782, 2.7261, 3.6745, 3.6469, 2.7261,\n",
      "         3.3714, 3.4297, 4.1500, 3.4297, 3.2601, 5.1321, 2.7261, 2.7261, 0.0000,\n",
      "         4.6947, 0.0000, 2.7261, 3.1384, 2.7261, 0.0000, 3.1581, 3.2584, 3.5656,\n",
      "         2.7261, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "for batch in runner.predict_loader(loader=itertools.islice(data['valid'], 1)):\n",
    "    print(batch['logits'].transpose(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
