{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:43:38.116751Z",
     "start_time": "2020-06-01T01:43:36.919790Z"
    }
   },
   "outputs": [],
   "source": [
    "from petastorm import make_batch_reader, TransformSpec\n",
    "from petastorm.pytorch import DataLoader as PetaDataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader as TorchDataLoader, IterableDataset\n",
    "from sklearn import preprocessing\n",
    "from collections import OrderedDict\n",
    "from torch import tensor\n",
    "import math\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:43:38.146749Z",
     "start_time": "2020-06-01T01:43:38.119513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching\n"
     ]
    }
   ],
   "source": [
    "FILE_PREFIX = 'file:'\n",
    "\n",
    "pre_open_fds = None\n",
    "def patch_leaking_fd():\n",
    "    global pre_open_fds\n",
    "    from pyarrow.parquet import ParquetFile, ParquetReader\n",
    "    def _patched_init(self, source, **kwargs):\n",
    "        self.source = source\n",
    "        return ParquetFile.__old_init__(self, source, **kwargs)\n",
    "\n",
    "    def _exit(self, *args, **kwargs):\n",
    "        if hasattr(self.source, 'close'):\n",
    "            self.source.close()\n",
    "            del self.source\n",
    "\n",
    "    def _bopen(fn):    \n",
    "        return open(fn, 'rb')\n",
    "\n",
    "    pre_open_fds = _bopen\n",
    "    if not hasattr(ParquetFile, '__old_init__'):\n",
    "        print(\"Patching\")\n",
    "        ParquetFile.__old_init__ = ParquetFile.__init__\n",
    "\n",
    "        ParquetFile.__init__ = _patched_init\n",
    "        ParquetFile.__exit__ = _exit\n",
    "        ParquetFile.__del__ = _exit\n",
    "\n",
    "    else:\n",
    "        print(\"Already patched\")\n",
    "\n",
    "patch_leaking_fd()\n",
    "\n",
    "\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "    def __init__(self, filename, rex=None):\n",
    "        super(MyIterableDataset).__init__()\n",
    "        self._filename_param = filename\n",
    "        self.filename = self._init_filenames(filename, rex)\n",
    "\n",
    "    def _init_filenames(self, filename, rex):\n",
    "        if rex is None:\n",
    "            return filename\n",
    "        \n",
    "        filename = filename[len(FILE_PREFIX):]\n",
    "        if not os.path.isdir(filename):\n",
    "            raise ValueError(f\"Filtering only possible for dirs, {filename} is not a one\")\n",
    "        paths = [os.path.join(dp, f) for dp, dn, fn in os.walk(filename) for f in fn]\n",
    "        res = list(map(\n",
    "            lambda f: FILE_PREFIX + f,\n",
    "            filter(lambda f: re.match(rex, f) is not None, paths)\n",
    "        ))\n",
    "        if (len(res) == 0):\n",
    "            raise ValueError(f\"0 files remained out ot {len(paths)} - seems regex is too restrictive\")\n",
    "\n",
    "        return res;\n",
    "\n",
    "    def _init_petaloader(self):\n",
    "        def _transform_row(df_batch):\n",
    "            return df_batch\n",
    "\n",
    "        transform = TransformSpec(_transform_row, removed_fields=['cat_id', 'store_id', 'state_id'])\n",
    "        reader = make_batch_reader(self.filename,\n",
    "                 schema_fields=['id', 'item_id', 'dept_id', 'cat_id', 'day_id',\n",
    "               'sales', 'day_date_str', 'month_id', 'date', 'wm_yr_wk',\n",
    "               'snap_flag', 'sell_price', 'sales_dollars', 'store_id', 'state_id'],\n",
    "                workers_count=1\n",
    "                #,transform_spec = transform\n",
    "        )\n",
    "        return PetaDataLoader(reader=reader, batch_size=128, shuffling_queue_capacity=100000)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1913*30490 # can be arbitrary large value to prevent WARN logs, seem to be ignored anyway\n",
    "\n",
    "    def __iter__(self):\n",
    "        print(f\"Iterator created on {self._filename_param}\")\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            count_cells = 0\n",
    "            count_batches = 0\n",
    "            with self._init_petaloader() as loader:\n",
    "                if pre_open_fds:\n",
    "                    loader.reader.dataset.fs.open = pre_open_fds\n",
    "                for batch in loader:\n",
    "                    count_batches += 1\n",
    "                    for price, sales_dollars in zip(batch['sell_price'], batch['sales_dollars']):\n",
    "                        price_is_nan = math.isnan(price)\n",
    "                        price_or_zero = 0. if price_is_nan else price\n",
    "                        count_cells += 1\n",
    "                        yield {'features': tensor([price_or_zero, price_is_nan]),\n",
    "                               'targets': tensor([sales_dollars])}\n",
    "                        \n",
    "            print(f'Done iterating: {count_batches} batches / ({count_cells} cells) ')\n",
    "        else:\n",
    "            raise ValueError(\"Not implemented for multithreading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:43:38.925446Z",
     "start_time": "2020-06-01T01:43:38.149213Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning:\n",
      "\n",
      "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.utils import set_global_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:43:38.935770Z",
     "start_time": "2020-06-01T01:43:38.929729Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED=42\n",
    "set_global_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:44:59.948578Z",
     "start_time": "2020-06-01T01:44:59.918841Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = 128\n",
    "\n",
    "train_ds = MyIterableDataset('file:./sales_series_melt.parquet', rex='.*/parquet_partition=(?!1/).*')\n",
    "valid_ds = MyIterableDataset('file:./sales_series_melt.parquet/parquet_partition=1')\n",
    "\n",
    "train_dl = TorchDataLoader(train_ds, batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "valid_dl = TorchDataLoader(valid_ds, batch_size=batch, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "data = OrderedDict()\n",
    "data[\"train\"] = train_dl\n",
    "data[\"valid\"] = valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:45:01.361711Z",
     "start_time": "2020-06-01T01:45:01.352391Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Net,self).__init__()\n",
    "        layers = [40, 20]\n",
    "        self.L1 = nn.Linear(num_features, layers[0])\n",
    "        torch.nn.init.xavier_uniform_(self.L1.weight) \n",
    "        torch.nn.init.zeros_(self.L1.bias)\n",
    "        \n",
    "        self.L2 = nn.Linear(layers[0], layers[1])\n",
    "        torch.nn.init.xavier_uniform_(self.L2.weight) \n",
    "        torch.nn.init.zeros_(self.L2.bias)\n",
    "        \n",
    "        self.L3 = nn.Linear(layers[1], 1)\n",
    "        torch.nn.init.xavier_uniform_(self.L3.weight) \n",
    "        torch.nn.init.zeros_(self.L3.bias)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.L1(x))\n",
    "        x = F.relu(self.L2(x))\n",
    "        x = F.relu(self.L3(x))\n",
    "        return x\n",
    "\n",
    "class MyLoss(nn.MSELoss):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inp, target):\n",
    "        return super().forward(inp, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:45:02.249922Z",
     "start_time": "2020-06-01T01:45:02.246240Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Net(num_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:45:03.239266Z",
     "start_time": "2020-06-01T01:45:03.235606Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = MyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T03:09:54.990353Z",
     "start_time": "2020-06-01T01:45:03.496985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator created on file:./sales_series_melt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning:\n",
      "\n",
      "Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/petastorm/workers_pool/thread_pool.py:187: PendingDeprecationWarning:\n",
      "\n",
      "isAlive() is deprecated, use is_alive() instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done iterating: 452642 batches / (57938071 cells) \n",
      "Iterator created on file:./sales_series_melt.parquet/parquet_partition=1\n",
      "Done iterating: 3042 batches / (389299 cells) \n",
      "[2020-05-31 20:09:54,970] \n",
      "1/1 * Epoch 1 (_base): lr=0.0100 | momentum=0.9000\n",
      "1/1 * Epoch 1 (train): loss=88.5450\n",
      "1/1 * Epoch 1 (valid): loss=88.5025\n",
      "Top best models:\n",
      "run/checkpoints/train.1.pth\t88.5025\n",
      "=> Loading checkpoint run/checkpoints/best_full.pth\n",
      "loaded state checkpoint run/checkpoints/best_full.pth (global epoch 1, epoch 1, stage train)\n"
     ]
    }
   ],
   "source": [
    "runner = SupervisedRunner()\n",
    "# /usr/local/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=data,\n",
    "    logdir=\"run\",\n",
    "    load_best_on_end=True,\n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:43:39.270890Z",
     "start_time": "2020-06-01T01:43:36.938Z"
    }
   },
   "outputs": [],
   "source": [
    "SupervisedRunner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:43:39.272018Z",
     "start_time": "2020-06-01T01:43:36.939Z"
    }
   },
   "outputs": [],
   "source": [
    "Runner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:43:39.273793Z",
     "start_time": "2020-06-01T01:43:36.941Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.dl import Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T01:43:39.275882Z",
     "start_time": "2020-06-01T01:43:36.942Z"
    }
   },
   "outputs": [],
   "source": [
    "make_batch_reader??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
