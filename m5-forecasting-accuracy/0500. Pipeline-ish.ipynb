{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:25:32.125164Z",
     "start_time": "2020-04-15T15:25:32.093548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:25:37.595673Z",
     "start_time": "2020-04-15T15:25:35.483964Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load ivanocode/pipeline/pipeline.py \n",
    "from ivanocode.pipeline.wrmsse import wrmsse_total, with_aggregate_series\n",
    "from ivanocode.ivanocommon import drop_level\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "raw = 'raw'\n",
    "processed = 'processed'\n",
    "submissions = 'submissions'\n",
    "tmp = 'tmp'\n",
    "\n",
    "n_total_series = 30490\n",
    "\n",
    "from joblib import Memory\n",
    "location = './tmp'\n",
    "memory = Memory(location, verbose=5)\n",
    "\n",
    "@memory.cache\n",
    "def read_series_sample(n_sample_series = 10):\n",
    "    sample_idx = set(np.random.choice(\n",
    "        range(1, n_total_series + 1),\n",
    "        n_sample_series\n",
    "    ))\n",
    "\n",
    "    # header\n",
    "    sample_idx.add(0)\n",
    "\n",
    "    return pd.read_csv(f'{raw}/sales_train_validation.csv', skiprows = lambda i: i not in sample_idx)\n",
    "\n",
    "@memory.cache\n",
    "def melt_sales_series(df_sales_train):\n",
    "    id_columns = [col for col in df_sales_train.columns if 'id' in col]\n",
    "    sales_columns = [col for col in df_sales_train.columns if 'd_' in col]\n",
    "    cat_columns = [col for col in id_columns if col != 'id']\n",
    "\n",
    "    df_sales_train_melt = df_sales_train.melt(\n",
    "        id_vars=id_columns,\n",
    "        var_name='day_id',\n",
    "        value_name='sales'\n",
    "    )\n",
    "\n",
    "    for col in id_columns:\n",
    "        df_sales_train_melt[col] = df_sales_train_melt[col].astype('category')\n",
    "\n",
    "    df_sales_train_melt['sales'] = df_sales_train_melt['sales'].astype('int16')\n",
    "\n",
    "    return df_sales_train_melt\n",
    "\n",
    "@memory.cache\n",
    "def extract_day_ids(df_sales_train_melt):\n",
    "    sales_columns = [f'd_{col}' for col in range(1,1913+1)]\n",
    "    mapping = {col: int(col.split('_')[1]) for col in sales_columns}\n",
    "    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    import datetime\n",
    "    d_1_date = pd.to_datetime('2011-01-29')\n",
    "    mapping = {day:(d_1_date + datetime.timedelta(days=day-1)) for day in range(1, 1913+1)}\n",
    "    df_sales_train_melt['day_date'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    mapping = {day:str((d_1_date + datetime.timedelta(days=day-1)).date()) for day in range(1, 1913+1)}\n",
    "    # gonna need it for joining with calendars & stuff\n",
    "    df_sales_train_melt['day_date_str'] = df_sales_train_melt['day_id'].map(mapping)\n",
    "\n",
    "    df_sales_train_melt['day_date_str'] = df_sales_train_melt['day_date_str'].astype('category')\n",
    "    df_sales_train_melt['day_id'] = df_sales_train_melt['day_id'].astype('int16')\n",
    "    df_sales_train_melt['month_id'] = df_sales_train_melt['day_date'].dt.month.astype('uint8')\n",
    "\n",
    "    return df_sales_train_melt\n",
    "\n",
    "@memory.cache\n",
    "def join_w_calendar(df_sales_train_melt):\n",
    "    df_calendar = pd.read_csv(f'{raw}/calendar.csv')\n",
    "    df_sales_train_melt =  df_sales_train_melt.merge(\n",
    "        df_calendar[['date', 'wm_yr_wk']],\n",
    "        left_on='day_date_str', right_on='date',\n",
    "        validate='many_to_one')\n",
    "\n",
    "    df_sales_train_melt['wm_yr_wk'] = df_sales_train_melt['wm_yr_wk'].astype('int16')\n",
    "    return df_sales_train_melt\n",
    "\n",
    "@memory.cache\n",
    "def join_w_prices(partition):\n",
    "    df_prices = pd.read_csv(f'{raw}/sell_prices.csv')\n",
    "    partition = partition.merge(\n",
    "        df_prices,\n",
    "        on=['store_id', 'item_id', 'wm_yr_wk'],\n",
    "        how='left'\n",
    "    )\n",
    "    partition['sell_price'] = partition['sell_price'].astype('float32')\n",
    "    partition['sales_dollars'] = (partition['sales'] * partition['sell_price']).astype('float32')\n",
    "    partition.fillna({'sales_dollars': 0}, inplace=True)\n",
    "    return partition\n",
    "\n",
    "from fastai.tabular import *\n",
    "\n",
    "def model_as_tabular(df_sales_train_melt):\n",
    "    df_sample = df_sales_train_melt.query('sales_dollars > 0')\n",
    "\n",
    "    day_ids = list(sorted(df_sample['day_id'].unique()))\n",
    "    valid_idx = np.flatnonzero(df_sample['day_id'] > 1000)\n",
    "\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    dep_var = 'sales_dollars'\n",
    "    cat_names = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'month_id', 'id']\n",
    "    cols = cat_names + ['sell_price'] + [dep_var]\n",
    "\n",
    "    path ='./tmp'\n",
    "    data = TabularDataBunch.from_df(path, df_sample[cols], dep_var, valid_idx=valid_idx,\n",
    "                                    procs=procs, cat_names=cat_names)\n",
    "\n",
    "    sales_range = df_sales_train_melt.agg({dep_var: ['min', 'max']})\n",
    "    learn = tabular_learner(data, layers=[1000,100], emb_szs=None, metrics=rmse, \n",
    "                        y_range=sales_range[dep_var].values)\n",
    "    #learn.lr_find()\n",
    "    #fig = learn.recorder.plot(return_fig=True)\n",
    "    #fig.savefig('lr_find.png')\n",
    "    learn.fit_one_cycle(1, 1e-1)\n",
    "    fig = learn.recorder.plot_losses(return_fig=True)\n",
    "    fig.savefig('loss_log.png')\n",
    "\n",
    "    \"\"\"\n",
    "    the above is pretty unstable, still we sort of got it to overfit slightly\n",
    "    epoch     train_loss  valid_loss  root_mean_squared_error  time    \n",
    "    0         83.365913   53.201424   7.188121                 00:00                                                                                      \n",
    "    1         56.507870   55.011505   7.313859                 00:00                                                                                      \n",
    "    2         48.014706   55.011673   7.313872                 00:00                                                                                      \n",
    "    3         44.242149   55.001785   7.313148                 00:00                                                                                      \n",
    "    4         42.347946   57.570885   7.462979                 00:00 \n",
    "    epoch     train_loss  valid_loss  root_mean_squared_error  time    \n",
    "    0         99.312691   88.176826   9.289624                 00:00                                                                                      \n",
    "    1         53.812771   45.480507   6.625162                 00:00                                                                                      \n",
    "    2         35.418938   19.952007   4.353195                 00:00                                                                                      \n",
    "    3         23.099392   17.516432   4.008787                 00:00                                                                                      \n",
    "    4         17.082275   17.641117   4.019526                 00:00 \n",
    "    \"\"\"\n",
    "    return learn, valid_idx\n",
    "\n",
    "sales_series = read_series_sample()\n",
    "sales_series = melt_sales_series(sales_series)\n",
    "sales_series = extract_day_ids(sales_series)\n",
    "sales_series = join_w_calendar(sales_series)\n",
    "sales_series = join_w_prices(sales_series)\n",
    "learn, valid_idx = model_as_tabular(sales_series)\n",
    "val = sales_series[sales_series.index.isin(valid_idx)]\n",
    "trn = sales_series[~sales_series.index.isin(valid_idx)]\n",
    "preds, y = learn.get_preds(DatasetType.Valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
