{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0.22.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('jigsaw/train.csv')\n",
    "test = pd.read_csv('jigsaw/test.csv')\n",
    "train['comment_text'] = train['comment_text'].fillna('')\n",
    "test['comment_text'] = test['comment_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(train['comment_text'])\n",
    "tokenizer.fit_on_texts(test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train['comment_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 4000\n",
    "train_sequences = pad_sequences(train_sequences, maxlen = SEQ_LEN)\n",
    "test_sequences =  pad_sequences(test_sequences,  maxlen = SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train['toxic,severe_toxic,obscene,threat,insult,identity_hate'.split(',')].as_matrix()\n",
    "\n",
    "validation_indexes = np.arange(0, 100)\n",
    "train_indexes = np.arange(100, 9581)\n",
    "train_sequences = train_sequences[train_indexes]\n",
    "train_labels = train_labels[train_indexes]\n",
    "validation_labels = train_labels[validation_indexes]\n",
    "validation_sequences = train_sequences[validation_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv1D, BatchNormalization, Dropout, Input, Dense, MaxPooling1D, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 47,  1, 43], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_docs)\n",
    "EMBEDDING_DIM = 3\n",
    "input_layer = Input(shape=(SEQ_LEN, 1))\n",
    "x = Conv1D(nb_filter = 100, filter_length = 3)(input_layer)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1000, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(train_labels.shape[1], activation='sigmoid')(x)\n",
    "model3 = Model(input_layer, output_layer)\n",
    "model3.compile(Adam(), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4000, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is amazing, why it's not on the doc\n",
    "# https://stackoverflow.com/questions/43396572/dimension-of-shape-in-conv1d/43399308#43399308\n",
    "np.expand_dims(train_sequences[0:1], axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.7196 - val_loss: 0.4963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f9bcf650>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.3465 - val_loss: 0.1858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f990a210>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.2126 - val_loss: 0.1139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f990a190>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.1883 - val_loss: 0.1107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f990a390>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.1697 - val_loss: 0.1004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9fb91db10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.1454 - val_loss: 0.0943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9fb93ae50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.1376 - val_loss: 0.0973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f9bcfd10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.1346 - val_loss: 0.0924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa6171ff50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.1304 - val_loss: 0.0809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f9bcfc10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "9481/9481 [==============================] - 53s - loss: 0.1247 - val_loss: 0.0828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f990a350>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.1230 - val_loss: 0.0830\n",
      "Epoch 2/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.1142 - val_loss: 0.0692\n",
      "Epoch 3/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.1122 - val_loss: 0.0721\n",
      "Epoch 4/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.1094 - val_loss: 0.0736\n",
      "Epoch 5/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.1055 - val_loss: 0.0657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f9bcf750>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=5, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_submission(model, name):\n",
    "    from IPython.display import FileLink\n",
    "    test_labels = model.predict(np.expand_dims(test_sequences, axis=2))\n",
    "    results = pd.DataFrame(test_labels, columns='toxic,severe_toxic,obscene,threat,insult,identity_hate'.split(','))\n",
    "    results['id'] = test['id']\n",
    "    results.to_csv('jigsaw/submission' + name + '.csv', index=False)\n",
    "    FileLink('jigsaw/submission' + name +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LB: 0.176, as good as word-level logistic regression\n",
    "prepare_submission(model3, '3_1')\n",
    "model3.save_weights('jigsaw/model3_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.1000 - val_loss: 0.0681\n",
      "Epoch 2/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.0928 - val_loss: 0.0579\n",
      "Epoch 3/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.0900 - val_loss: 0.0601\n",
      "Epoch 4/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.0857 - val_loss: 0.0735\n",
      "Epoch 5/5\n",
      "9481/9481 [==============================] - 53s - loss: 0.0841 - val_loss: 0.0534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9f9fb56d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=5, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3.save_weights('jigsaw/model3_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LB: 0.191\n",
    "prepare_submission(model3, '3_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_docs)\n",
    "EMBEDDING_DIM = 3\n",
    "input_layer = Input(shape=(SEQ_LEN, 1))\n",
    "x = Conv1D(nb_filter = 100, filter_length = 3)(input_layer)\n",
    "x = Conv1D(nb_filter = 100, filter_length = 3)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(train_labels.shape[1], activation='sigmoid')(x)\n",
    "model4 = Model(input_layer, output_layer)\n",
    "model4.compile(Adam(), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9481 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "9481/9481 [==============================] - 77s - loss: 0.6989 - val_loss: 0.4134\n",
      "Epoch 2/5\n",
      "9481/9481 [==============================] - 77s - loss: 0.2917 - val_loss: 0.2211\n",
      "Epoch 3/5\n",
      "9481/9481 [==============================] - 77s - loss: 0.1681 - val_loss: 0.1625\n",
      "Epoch 4/5\n",
      "9481/9481 [==============================] - 78s - loss: 0.1522 - val_loss: 0.1290\n",
      "Epoch 5/5\n",
      "9481/9481 [==============================] - 78s - loss: 0.1480 - val_loss: 0.1041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fee2c085450>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(np.expand_dims(train_sequences, axis=2), train_labels,\n",
    "           validation_data=(np.expand_dims(validation_sequences, axis=2), validation_labels), \n",
    "           nb_epoch=5, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LB: 0.25 \n",
    "prepare_submission(model4, '4_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: explore which examples were mistagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sudo apt-get install software-properties-common\n",
    "# sudo add-apt-repository ppa:git-core/ppa\n",
    "# sudo apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
