{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from lxml import html\n",
    "from lxml import etree\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def identify_letter_urls(blog_url):\n",
    "    response = requests.get(blog_url)\n",
    "    tree = html.fromstring(response.content)\n",
    "    hrefs = tree.xpath('//dt[@class=\"entry-title\"]/a[@class=\"subj-link\"]')\n",
    "    hrefs = filter(lambda href: href.text.startswith(\"Письмо.\"), hrefs)\n",
    "    pages_to_visit = map(lambda href: href.get('href'), hrefs)\n",
    "    return list(pages_to_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pages_to_visit = identify_letter_urls(\"https://evo-lutio.livejournal.com/?tag=evolutiolab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_letter(post_url):\n",
    "    response = requests.get(post_url)\n",
    "    tree = html.fromstring(response.content)\n",
    "    article = tree.xpath('//div[@class=\"b-singlepost-bodywrapper\"]/article')[0]\n",
    "    lj_authors = article.xpath(\"span/a/b/text()\")\n",
    "    lj_author = str(lj_authors[0])\n",
    "    lj_author2 = str(lj_authors[1]) if len(lj_authors) > 1 else None\n",
    "\n",
    "    lj_author_cited = False\n",
    "    text = u''\n",
    "    for el in article.itertext():\n",
    "        element_text = str(el)\n",
    "        if (element_text == lj_author2):\n",
    "            return text\n",
    "        if (element_text == lj_author):\n",
    "            lj_author_cited = True\n",
    "            continue\n",
    "        if (not lj_author_cited):\n",
    "            continue\n",
    "        if (element_text.startswith(\"(\")):\n",
    "            continue\n",
    "        text += element_text + '\\n'\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 46 letter urls now\n",
      "Got 81 letter urls now\n",
      "Got 120 letter urls now\n",
      "Got 160 letter urls now\n",
      "Got 195 letter urls now\n",
      "Got 229 letter urls now\n",
      "Got 264 letter urls now\n",
      "Got 300 letter urls now\n",
      "Got 333 letter urls now\n",
      "Got 356 letter urls now\n",
      "Stuck at page 10 quitting\n"
     ]
    }
   ],
   "source": [
    "from sets import Set\n",
    "delta = 40\n",
    "letter_urls = Set()\n",
    "for page in range(0, 20):\n",
    "    prev_size = len(letter_urls)\n",
    "    letter_urls.update(Set(identify_letter_urls(\"https://evo-lutio.livejournal.com/?tag=evolutiolab&skip=\" + str(delta*page))))\n",
    "    if (prev_size == len(letter_urls)):\n",
    "        print(\"Stuck at page \" + str(page) + \" quitting\")\n",
    "        break;\n",
    "    else:\n",
    "        print(\"Got \" + str(len(letter_urls)) + \" letter urls now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir -p letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letter_urls = list(letter_urls)\n",
    "f = open(\"letter_urls.txt\", \"w\")\n",
    "f.write(\"\\n\".join(map(lambda x: str(x), letter_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letter #567581 already extracted - skipping\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for letter_url in letter_urls:\n",
    "    letter_id = re.findall(\"([0-9]+).html\", letter_url)[0]\n",
    "    letter_path = \"letters/{letter_id}.txt\".format(letter_id=letter_id)\n",
    "    if (os.path.exists(letter_path)):\n",
    "        print(\"Letter #{letter_id} already extracted - skipping\".format(letter_id=letter_id))\n",
    "        continue\n",
    "\n",
    "    letter_text = extract_letter(letter_url)\n",
    "    with open(letter_path, \"w\") as f:        \n",
    "        f.write(letter_text)\n",
    "    \n",
    "    print(\"Extracted letter {letter_id}: {length} length\".format(letter_id=letter_id, length=len(letter_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0M\tletters\r\n"
     ]
    }
   ],
   "source": [
    "!du -hs letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318972.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls letters | head -n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "all_text = '';\n",
    "for fname in os.listdir(\"letters\"):\n",
    "    with open('letters/'+fname, 'r') as f:\n",
    "        all_text += f.read() + '\\0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(all_text)))\n",
    "chars.insert(0, '\\0')\n",
    "vocab_size = len(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5501573"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nb sequences:', 5501534)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(idx) - maxlen+1):\n",
    "    sentences.append(idx[i: i + maxlen])\n",
    "    next_chars.append(idx[i+1: i+maxlen+1])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = np.concatenate([[np.array(o)] for o in sentences[:-2]])\n",
    "next_chars = np.concatenate([[np.array(o)] for o in next_chars[:-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5501532, 40), (5501532, 40))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sentences.shape, next_chars.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_sample = sentences[0:50000]\n",
    "next_chars_sample = next_chars[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fac = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, LSTM, TimeDistributed, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=maxlen),\n",
    "        LSTM(512, input_dim=n_fac,return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        LSTM(512, return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size)),\n",
    "        Activation('softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 209s - loss: 1.7503   \n",
      "CPU times: user 2min 49s, sys: 1min 8s, total: 3min 58s\n",
      "Wall time: 4min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63695aad90>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(sentences_sample, np.expand_dims(next_chars_sample,-1), batch_size=64, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 121s - loss: 1.7616   \n",
      "CPU times: user 1min 54s, sys: 30.8 s, total: 2min 24s\n",
      "Wall time: 2min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f634bd73cd0>"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consume_less=gpu reports to have spent less time = 119s vs 209s :/\n",
    "%time model.fit(sentences_sample, np.expand_dims(next_chars_sample,-1), batch_size=64, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "def print_example(symbols=1000):\n",
    "    seed_string=\"Здравствуйте, Эволюция.\"\n",
    "    for i in range(symbols):\n",
    "        try:\n",
    "            x=np.array([char_indices[c] for c in seed_string[-40:]])[np.newaxis,:]\n",
    "            preds = model.predict(x, verbose=0)[0][-1]\n",
    "            preds = preds/np.sum(preds)\n",
    "            next_char = choice(chars, p=preds)\n",
    "            seed_string = seed_string + next_char\n",
    "        except:            \n",
    "            print(\"X \" + str(x.shape))\n",
    "            print(\"Preds \" + str(len(preds)))\n",
    "            print(\"Chars \" + str(len(chars)))\n",
    "            print(\"i \" + str(i))\n",
    "            raise\n",
    "    print(seed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 121s - loss: 1.2714   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f63495d8050>"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences_sample, np.expand_dims(next_chars_sample,-1), batch_size=64, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте, Эволюция.\n",
      "На стал я у меня :Чыхор ен сельа бод не хаз кела ток гомуше и не нав денерения опять лоторого вв сраси ра на хотиения в иехил «дс. Вень.\n",
      "У пусно полилалым. Он ходло рямы, жажесклу�\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 121s - loss: 1.0868   \n",
      "Здравствуйте, Эволюция.\n",
      "Потьпить с антересный невелю. Нет, так? На тро «не опять прямшопе рыла мы симьмились, и остраницуся. . \n",
      "\n",
      " Чсастая что ответила сне, лак-м не раз иткатил, чтосут кад это не довегще �\n"
     ]
    }
   ],
   "source": [
    "model.fit(sentences_sample, np.expand_dims(next_chars_sample,-1), batch_size=64, nb_epoch=1)\n",
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 121s - loss: 0.9418   \n",
      "Здравствуйте, Эволюция.\n",
      "Про себя (мне 23, едс находили увобо умадал, я его дат-иние, написал что мружым снова в понр-дения по ваю другую с денте объеки в лаписала воздух Притивинось, тебя пришло!\n",
      "Простил\n"
     ]
    }
   ],
   "source": [
    "model.fit(sentences_sample, np.expand_dims(next_chars_sample,-1), batch_size=64, nb_epoch=1)\n",
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 121s - loss: 0.8359   \n",
      "Здравствуйте, Эволюция.\n",
      "\n",
      "Ссе это были понять встречи, антерес с упать с ним бы вышла не умержёю помучтились встретиться ,же мосяца вообще она извинее не ответила. Через неша обо родиловала пок-ли все �\n"
     ]
    }
   ],
   "source": [
    "model.fit(sentences_sample, np.expand_dims(next_chars_sample,-1), batch_size=64, nb_epoch=1)\n",
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 121s - loss: 0.7554   \n",
      "Здравствуйте, Эволюция.\n",
      "Сечером заерал привдах потом стал ла приедет своджели домой и все –такие нет, говорили комплиментов. На привело переписывался на разу. \n",
      "Я нипрадямать?\n",
      "-Волчал по жимнье дела» \n"
     ]
    }
   ],
   "source": [
    "model.fit(sentences_sample, np.expand_dims(next_chars_sample,-1), batch_size=64, nb_epoch=1)\n",
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 111s - loss: 0.6865 - val_loss: 0.7039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6350b9b790>"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences_sample, np.expand_dims(next_chars_sample,-1), batch_size=64, nb_epoch=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте, Эволюция.\n",
      "\n",
      "Извиняться вечером 9го, познакомились и ушла. Да и все ответила. Разводиться, когда я не реагились е возде другую ком того в зафе. На помторится, когда от мужем потом заехал на �\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5226455 samples, validate on 275077 samples\n",
      "Epoch 1/1\n",
      "5226455/5226455 [==============================] - 12748s - loss: 0.9015 - val_loss: 0.8422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6351091850>"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences, np.expand_dims(next_chars,-1), batch_size=64, nb_epoch=1, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте, Эволюция.\n",
      "Когда я видимо. Ланс его причину. А просто мы тоже и написывались. Спросила М. Не отказался, по-других образованиях, был еду на вопрос, когда все это время пока во всем очень айс�\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('lesson4_evo_rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте, Эволюция.\n",
      "Вы потеком деньги свою реакцию. Её месте слушай, квы закрыло длительных любви. Когда мне понял, что он был все же мафии, этот усопенность, блружен не приносит, сколько дней я сд�\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте, Эволюция.\n",
      "Просто было в профессию - зателениями не мог наблюдать и тсуборгум, но ничего не хватать на таком фраз безразвитывай уже не в ресторане, ладно же был куда-то развивается подруг, звонить по историю, через незнакомства, в срок, как у меня были после него было, что не до него пришлось, так доктора раз в или не состояния и не знаю, всё задумалась, если говорили, что если всё хорошо, и это ему без другой стороны.\n",
      " (Заботы, и спросил такие проектов, а ну так правильно сиделись к нему, с ребенком, я была с ним, не хотела бы это вернуть Вам дорог(провел и ничего не его реально было я учусь, мы тоже пошли 5 недель ваши поведение подействительно он написал мне, что я выразила себя купила девушным, скрывается экзутства, заплачивать без нашего заниматься звонки. Нужны со мной и не дожду. \n",
      "Он ответил, как раз еще к почту очень дурацкого: \n",
      "Общение заканчивала корону:. Захочешь. Я захочу, я предовольна за мужчинами, как чтобы появилось тащить ночью. \n",
      "Все ч не звонит потом написал что у него сексом, но наверное, так как говорил, что не верное, поехала по выходные с бизнесом в направлении, что мне каталось в мессенд�\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте, Эволюция.\n",
      "Когда мы были все предложили об отношениях, она спрашивала уже как раз почему-то родители. Я заказала себя не могла достаточно приехать отношения и не пугает это слегкие видо�\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('lesson4_evo_rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте, Эволюция.\n",
      "Мне 38 и в три часов, потом был интересное месяц к другому денег внимания на кологичный тоже писал, как я, и я его представляла неделю на мою головку, и думала ему, что не помню, ч�\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5226455 samples, validate on 275077 samples\n",
      "Epoch 1/1\n",
      "5226455/5226455 [==============================] - 6280s - loss: 0.8465 - val_loss: 0.8271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b9b23f3d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences, np.expand_dims(next_chars,-1), batch_size=2048, nb_epoch=1, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the above is cool: batch size 64 -> 2048 led to 2x speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте, Эволюция.\n",
      "И я все ей пообщалась, но сделала долго, не бывает… так как для этого, мое пляжь выдавила так девушки в растворились и не вижу, он не за то, что и мне романтично поехать, поняла, �\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CudaNdarrayType(float32, scalar)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте, Эволюция.\n",
      "Энергичичными переписками? А мне - хотя бы хотелось бы все семью, а не интересная - что я точно ничего не смог заигрывать. В этот первого времени постоянно должны понять, что включается. Мне 48, встретились кмкую-то отношениях? \n",
      "Сказал, что у нас назвал в муж, они очень пошли, чтобы сидеть жить, мир это чаще вообще не потерял вполне, наверное, я старалась паузу, замкнули, не часто еще лень на эту переписку и пыталась довольно рада, что меня наверное, разговор жизни, впрочем, как как очень элементы и порыдаешь тебе в достаточно сейчас, то не в хорошие л�\n"
     ]
    }
   ],
   "source": [
    "print_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('lesson4_evo_rnn_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to /home/ubuntu/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "красивый мама красиво мыть рама\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install pymystem3\n",
    "from pymystem3 import Mystem\n",
    "text = \"Красивая мама красиво мыла раму\"\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget http://rusvectores.org/static/models/rusvectores2/news_rusvectores2.bin.gz\n",
    "#!wget http://rusvectores.org/static/models/ruscorpora_1_300_10.bin.gz\n",
    "#!wget https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz?dl=0 -O ru_Kyubyong_fasttext.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install word2vec\n",
    "#!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this thing crashes\n",
    "#import gensim\n",
    "#res_model = gensim.models.KeyedVectors.load_word2vec_format('ruscorpora_1_300_10.bin.gz', binary=True)\n",
    "#from gensim.models.word2vec import Word2Vec\n",
    "#import gensim.downloader as api\n",
    "#corpus = api.load('word2vec-ruscorpora-300')  # download the corpus and return it opened as an iterable\n",
    "#model = Word2Vec(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "la -la ruscorpora_1_300_10.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "with open('news_rusvectores2.bin', 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf8' codec can't decode byte 0x8d in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c693776918ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'news_rusvectores2.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(lines[15])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/word2vec/io.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(fname, kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not identify kind'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'txt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/word2vec/wordvectors.pyc\u001b[0m in \u001b[0;36mfrom_binary\u001b[0;34m(cls, fname, vocabUnicodeSize, desired_vocab, encoding, newLines)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0minclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesired_vocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdesired_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                     \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;31m# read vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/encodings/utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0x8d in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "import word2vec\n",
    "word2vec.load('news_rusvectores2.bin', kind='bin')\n",
    "#print(lines[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "download_file_from_google_drive('0B0ZXk88koS2KMUJxZ0w0WjRGdnc', 'word2vec_Kyubyong.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  word2vec_Kyubyong.zip\n",
      "  inflating: ru.bin                  \n",
      "  inflating: ru.tsv                  \n",
      "  inflating: ru.bin.syn1neg.npy      \n",
      "  inflating: ru.bin.syn0.npy         \n"
     ]
    }
   ],
   "source": [
    "!unzip word2vec_Kyubyong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# git clone https://github.com/facebookresearch/fastText.git\n",
    "# sudo apt-get install cmake -y\n",
    "# cd fastText\n",
    "# mkdir build && cd build && cmake ..\n",
    "# make && sudo make install\n",
    "# pip install pybind pybind11\n",
    "# python setup.py install\n",
    "# pip install fasttext --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "fastText: cannot load data.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-dc5e34dc7511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Skipgram model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;31m# list of words in dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfasttext/fasttext.pyx\u001b[0m in \u001b[0;36mfasttext.fasttext.skipgram (fasttext/fasttext.cpp:5886)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfasttext/fasttext.pyx\u001b[0m in \u001b[0;36mfasttext.fasttext.train_wrapper (fasttext/fasttext.cpp:4770)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: fastText: cannot load data.txt"
     ]
    }
   ],
   "source": [
    "# TODO: try this https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "print model.words # list of words in dictionary\n",
    "\n",
    "# CBOW model\n",
    "model = fasttext.cbow('data.txt', 'model')\n",
    "print model.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru.bin\n",
      "ru.vec\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf ru_Kyubyong_fasttext.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru_Kyubyong_fasttext.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls ru_Kyubyong_fasttext*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Skipgram model\n",
    "model = fasttext.skipgram('ru.vec', 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
